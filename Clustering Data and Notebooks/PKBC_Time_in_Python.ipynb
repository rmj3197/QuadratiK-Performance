{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from - https://stackoverflow.com/questions/3103178/how-to-get-the-system-info-with-python\n",
    "\n",
    "import platform,socket,re,uuid,json,psutil,logging\n",
    "\n",
    "def getSystemInfo():\n",
    "    try:\n",
    "        info={}\n",
    "        info['platform']=platform.system()\n",
    "        info['platform-release']=platform.release()\n",
    "        info['platform-version']=platform.version()\n",
    "        info['architecture']=platform.machine()\n",
    "        info['hostname']=socket.gethostname()\n",
    "        info['ip-address']=socket.gethostbyname(socket.gethostname())\n",
    "        info['mac-address']=':'.join(re.findall('..', '%012x' % uuid.getnode()))\n",
    "        info['processor']=platform.processor()\n",
    "        info['ram']=str(round(psutil.virtual_memory().total / (1024.0 **3)))+\" GB\"\n",
    "        return json.dumps(info)\n",
    "    except Exception as e:\n",
    "        logging.exception(e)\n",
    "\n",
    "json.loads(getSystemInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_cluster_cosine_similarity(X, y):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between centroids of clusters in X defined by labels y.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix of shape (n_samples, n_features)\n",
    "    y (numpy.ndarray): Cluster labels of shape (n_samples,)\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Cosine similarity matrix between centroids\n",
    "    \"\"\"\n",
    "    unique_labels = np.unique(y)\n",
    "    centroids = np.array([X[y == label].mean(axis=0) for label in unique_labels])\n",
    "    \n",
    "    return cosine_similarity(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[Generating Embeddings](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tiktoken  # For token counting\n",
    "import ast\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"SET KEY\"\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "# Function to get embeddings from OpenAI API\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    # Replace newlines with spaces for consistent processing\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return (\n",
    "        client.embeddings.create(\n",
    "            input=[text], model=model, dimensions=\"INPUT YOUR DIMENSION\"\n",
    "        )\n",
    "        .data[0]\n",
    "        .embedding\n",
    "    )\n",
    "\n",
    "# Load dataset\n",
    "df = \"THIS SHOULD BE YOUR DATASET\"\n",
    "\n",
    "# Set maximum tokens and encoding parameters\n",
    "max_tokens = 7000\n",
    "embedding_encoding = \"cl100k_base\"\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n",
    "\n",
    "# Count tokens in each text\n",
    "df[\"n_tokens\"] = df[\"text\"].apply(lambda x: len(encoding.encode(x)))\n",
    "\n",
    "\n",
    "# Function to truncate text to max token length\n",
    "def truncate_text(text, encoding, max_tokens=7000):\n",
    "    tokens = encoding.encode(text)\n",
    "    if len(tokens) > max_tokens:\n",
    "        # If text is too long, truncate to max_tokens\n",
    "        tokens = tokens[:max_tokens]\n",
    "        return encoding.decode(tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Truncate texts that are too long\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: truncate_text(x, encoding, max_tokens))\n",
    "\n",
    "# Recount tokens after truncation\n",
    "df[\"n_tokens\"] = df[\"text\"].apply(lambda x: len(encoding.encode(x)))\n",
    "\n",
    "# Get embeddings for each text using OpenAI API\n",
    "df[\"ada_embedding\"] = df[\"text\"].apply(\n",
    "    lambda x: get_embedding(x, model=\"text-embedding-3-small\")\n",
    ")\n",
    "\n",
    "# Save embeddings to CSV file\n",
    "df.to_csv(\"SAVE YOUR EMBEDDINGS WITH FILE NAME\", index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Different Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Dictionary to store datasets and their true cluster counts\n",
    "dataset_info = {\n",
    "    'digits': {'n_clusters': 10, 'path': 'digits_embeddings.csv'},\n",
    "    'birch': {'n_clusters': 3, 'path': 'birch_dataset.csv'},\n",
    "    '3Newsgroups': {'n_clusters': 3, 'path': 'embeddings_news_256.csv'},\n",
    "    '20Newsgroups': {'n_clusters': 20, 'path': 'embeddings_news_all_256.csv'},\n",
    "    'stackexchange': {'n_clusters': 15, 'path': 'embeddings_stk_128.csv'},\n",
    "    'reddit': {'n_clusters': 19, 'path': 'reddit_384.csv'},\n",
    "    'detectai': {'n_clusters': 2, 'path': 'embeddings_ai_detect_32.csv'},\n",
    "    'arxiv': {'n_clusters': 11, 'path': 'embeddings_arxiv_test_128.csv'},\n",
    "}\n",
    "\n",
    "# Load datasets into a dictionary\n",
    "datasets = {}\n",
    "for name, info in dataset_info.items():\n",
    "    try:\n",
    "        data = pd.read_csv(info['path'])\n",
    "        datasets[name] = {\n",
    "            'data': data,\n",
    "            'n_clusters': info['n_clusters']\n",
    "        }\n",
    "        print(f\"Successfully loaded {name} dataset with {info['n_clusters']} clusters\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {name} dataset: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import time\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from QuadratiK.spherical_clustering import PKBC\n",
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    v_measure_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "def return_embeddings(x):\n",
    "    \"\"\"Convert string representation of lists into actual lists.\"\"\"\n",
    "    return ast.literal_eval(x)\n",
    "\n",
    "def evaluate_pkbc_clustering(dataset_info, num_iterations=1):\n",
    "    \"\"\"Function to perform PKBC clustering on multiple datasets multiple times and evaluate performance.\"\"\"\n",
    "    performance_metrics = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        print(f\"Iteration {i+1}/{num_iterations}\")\n",
    "        \n",
    "        for name, info in dataset_info.items():\n",
    "            try:\n",
    "                df = pd.read_csv(info['path'])\n",
    "                \n",
    "                # Assign labels for the Birch dataset manually and skip return_embeddings\n",
    "                if name == 'birch':\n",
    "                    df[\"labels\"] = [1] * 100000 + [2] * 100000 + [3] * 100000\n",
    "                    embeddings = np.array(df.drop(columns=[\"labels\"]))  # Assuming all remaining columns are embeddings\n",
    "                elif name == 'digits':\n",
    "                    embeddings = np.array(df.drop(columns=[\"labels\"]))  # No return_embeddings applied\n",
    "                else:\n",
    "                    df[\"embedding\"] = df[\"ada_embedding\"].apply(return_embeddings)\n",
    "                    embeddings = np.array(df[\"embedding\"].tolist())\n",
    "                \n",
    "                true_k = info['n_clusters']\n",
    "                \n",
    "                print(f\"Processing {name} dataset with {true_k} clusters...\")\n",
    "                \n",
    "                start_time = time.time()\n",
    "                pkbc = PKBC(num_clust=true_k, random_state=42).fit(embeddings)\n",
    "                end_time = time.time()\n",
    "                \n",
    "                # Compute ARI and V-measure\n",
    "                ari_pkbc = adjusted_rand_score(pkbc.labels_[true_k], df[\"labels\"].tolist())\n",
    "                vscore_pkbc = v_measure_score(pkbc.labels_[true_k], df[\"labels\"].tolist())\n",
    "                \n",
    "                # Compute confusion matrix and map predicted labels to true labels\n",
    "                conf_mat = confusion_matrix(df[\"labels\"].tolist(), pkbc.labels_[true_k])\n",
    "                row_ind, col_ind = linear_sum_assignment(-conf_mat)  # Maximize matches\n",
    "                label_mapping = {col_ind[i]: i for i in range(len(col_ind))}\n",
    "                mapped_labels = np.array([label_mapping[label] for label in pkbc.labels_[true_k]])\n",
    "                \n",
    "                # Compute precision and recall\n",
    "                precision_pkbc, recall_pkbc, _, _ = precision_recall_fscore_support(\n",
    "                    df[\"labels\"].tolist(), mapped_labels, average=\"macro\"\n",
    "                )\n",
    "                \n",
    "                # Store results\n",
    "                performance_metrics.append({\n",
    "                    \"Iteration\": i+1,\n",
    "                    \"Dataset\": name,\n",
    "                    \"Algorithm\": \"PKBC\",\n",
    "                    \"ARI\": ari_pkbc,\n",
    "                    \"V Measure\": vscore_pkbc,\n",
    "                    \"Macro Precision\": precision_pkbc,\n",
    "                    \"Macro Recall\": recall_pkbc,\n",
    "                    \"Number of Rows\": embeddings.shape[0],\n",
    "                    \"Number of Columns\": embeddings.shape[1],\n",
    "                    \"Computation Time (seconds)\": end_time - start_time,\n",
    "                    \"K\": true_k\n",
    "                })\n",
    "                \n",
    "                print(f\"Completed {name} dataset in iteration {i+1}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {name} dataset in iteration {i+1}: {str(e)}\")\n",
    "    \n",
    "    # Convert results into DataFrame\n",
    "    return pd.DataFrame(performance_metrics)\n",
    "\n",
    "# Run the function and display the results\n",
    "# results_df = evaluate_pkbc_clustering(dataset_info, num_iterations=10)\n",
    "# print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to compute Cosine Similarity of the Centroids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_results = {}\n",
    "\n",
    "for name, info in datasets.items():\n",
    "    try:\n",
    "        df = info['data']\n",
    "        \n",
    "        # Assign labels for the Birch dataset manually and skip return_embeddings\n",
    "        if name == 'birch':\n",
    "            df[\"labels\"] = [1] * 100000 + [2] * 100000 + [3] * 100000\n",
    "            embeddings = np.array(df.drop(columns=[\"labels\"]))  # Assuming all remaining columns are embeddings\n",
    "        elif name == 'digits':\n",
    "            embeddings = np.array(df.drop(columns=[\"labels\"]))  # No return_embeddings applied\n",
    "        else:\n",
    "            df[\"embedding\"] = df[\"ada_embedding\"].apply(return_embeddings)\n",
    "            embeddings = np.array(df[\"embedding\"].tolist())\n",
    "        \n",
    "        cosine_similarity_results[name] = compute_cluster_cosine_similarity(embeddings, df[\"labels\"].tolist())\n",
    "        print(f\"Computed cosine similarity for {name} dataset.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing cosine similarity for {name} dataset: {str(e)}\")\n",
    "\n",
    "# Display the results\n",
    "cosine_similarity_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
